{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9533103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa2f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Otsu's thresholding\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB), gray, binary, contour\n",
    "\n",
    "\n",
    "# Resample boundary to 256 points\n",
    "def resample_points(contour, n_points=256):\n",
    "    contour = contour.squeeze()  # Ensure contour is a 2D array\n",
    "    if contour.ndim == 3:\n",
    "        contour = contour.reshape(-1, 2)  # Reshape to (n, 2) if it's (n, 1, 2)\n",
    "    elif contour.ndim == 1:\n",
    "        contour = contour.reshape(-1, 2)  # Reshape to (n, 2) if it's (2n,)\n",
    "    \n",
    "    distances = np.cumsum(np.sqrt(np.sum(np.diff(contour, axis=0)**2, axis=1)))\n",
    "    distances = np.insert(distances, 0, 0)\n",
    "    total_length = distances[-1]\n",
    "    spacing = np.linspace(0, total_length, n_points)\n",
    "    return np.array([np.interp(spacing, distances, contour[:, i]) for i in [0, 1]]).T.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5cd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtd_feature(contour, N=2048, M=5):\n",
    "    # Sample the contour evenly into N points\n",
    "    sampled_points = resample_points(contour, N)\n",
    "    Ts = int(np.floor(np.log2(N/2)))\n",
    "    \n",
    "    # Initialize feature arrays\n",
    "    alpha = np.zeros((N, Ts))\n",
    "    beta = np.zeros((N, Ts))\n",
    "    gamma = np.zeros((N, Ts))\n",
    "\n",
    "    for i in range(N):\n",
    "        Pi = sampled_points[i]\n",
    "        for k in range(1, Ts + 1):\n",
    "            d_k = 2**(k-1)\n",
    "            Pi_plus_d_k = sampled_points[(i + d_k) % N]\n",
    "            Pi_minus_d_k = sampled_points[(i - d_k - 1) % N]\n",
    "\n",
    "            # Calculate signed area\n",
    "            t_sa = 0.5 * np.linalg.det(np.array([\n",
    "                [Pi_minus_d_k[0], Pi_minus_d_k[1], 1],\n",
    "                [Pi[0], Pi[1], 1],\n",
    "                [Pi_plus_d_k[0], Pi_plus_d_k[1], 1]\n",
    "            ]))\n",
    "            alpha[i, k-1] = np.abs(t_sa)\n",
    "            beta[i, k-1] = 1 if t_sa >= 0 else 0\n",
    "\n",
    "            # Calculate center distance\n",
    "            centroid = (Pi_minus_d_k + Pi + Pi_plus_d_k) / 3\n",
    "            gamma[i, k-1] = np.sqrt((Pi[0] - centroid[0])**2 + (Pi[1] - centroid[1])**2)\n",
    "\n",
    "    # Normalize alpha and gamma features column-wise to achieve scale invariance.\n",
    "    for k in range(Ts):\n",
    "        max_alpha = np.max(np.abs(alpha[:, k]))\n",
    "        if max_alpha != 0:\n",
    "            alpha[:, k] /= max_alpha\n",
    "        max_gamma = np.max(np.abs(gamma[:, k]))\n",
    "        if max_gamma != 0:\n",
    "            gamma[:, k] /= max_gamma\n",
    "    \n",
    "    # Apply discrete Fourier transform (DFT) on each feature (each column) along the contour axis.\n",
    "    # We take the amplitude (absolute values) of the first M coefficients.\n",
    "    alpha_fft = np.zeros((M, Ts))\n",
    "    beta_fft = np.zeros((M, Ts))\n",
    "    gamma_fft = np.zeros((M, Ts))\n",
    "    \n",
    "    for k in range(Ts):\n",
    "        fft_alpha = np.fft.fft(alpha[:, k])\n",
    "        fft_beta = np.fft.fft(beta[:, k])\n",
    "        fft_gamma = np.fft.fft(gamma[:, k])\n",
    "        # Retain the first M low-frequency components\n",
    "        alpha_fft[:, k] = np.abs(fft_alpha[:M])\n",
    "        beta_fft[:, k] = np.abs(fft_beta[:M])\n",
    "        gamma_fft[:, k] = np.abs(fft_gamma[:M])\n",
    "    \n",
    "    # Concatenate the features (flatten each feature type and then concatenate)\n",
    "    # Final feature dimension: 3 * (M * Ts)\n",
    "    mtd_descriptor = np.concatenate((alpha_fft.flatten(),\n",
    "                                     beta_fft.flatten(),\n",
    "                                     gamma_fft.flatten()))\n",
    "    return mtd_descriptor\n",
    "\n",
    "# Example usage:\n",
    "# mtd_features = mtd_feature(query_contour)\n",
    "# print(mtd_features)\n",
    "# print(mtd_features.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5439f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbp_hf_feature(gray_image, P=24, R=3):\n",
    "    # Calculate LBP\n",
    "    lbp_image = local_binary_pattern(gray_image, P, R, method='uniform')\n",
    "    \n",
    "    # Helper functions\n",
    "    def code_to_binary_tuple(code, P):\n",
    "        code = int(code)\n",
    "        return tuple((code >> np.arange(P)) & 1)\n",
    "    \n",
    "    def is_uniform(binary_tuple):\n",
    "        transitions = sum(binary_tuple[i] != binary_tuple[(i+1) % len(binary_tuple)] for i in range(len(binary_tuple)))\n",
    "        return transitions <= 2\n",
    "    \n",
    "    def rotation_index(binary_tuple):\n",
    "        best = binary_tuple\n",
    "        best_shift = 0\n",
    "        for s in range(1, len(binary_tuple)):\n",
    "            shifted = binary_tuple[s:] + binary_tuple[:s]\n",
    "            if shifted < best:\n",
    "                best = shifted\n",
    "                best_shift = s\n",
    "        return best_shift\n",
    "    \n",
    "    # Build a lookup dictionary for unique codes\n",
    "    unique_codes = np.unique(lbp_image)\n",
    "    uniform_map = {}\n",
    "    for code in unique_codes:\n",
    "        b = code_to_binary_tuple(code, P)\n",
    "        if is_uniform(b):\n",
    "            n = sum(b)\n",
    "            r = rotation_index(b)\n",
    "            uniform_map[int(code)] = ('uniform', n, r)\n",
    "        else:\n",
    "            uniform_map[int(code)] = ('nonuniform', None, None)\n",
    "    \n",
    "    # Initialize histogram for uniform patterns (for n = 1,...,P-1)\n",
    "    H_uniform = {n: np.zeros(P, dtype=int) for n in range(1, P)}\n",
    "    count_zero = 0\n",
    "    count_allone = 0\n",
    "    count_nonuniform = 0\n",
    "    \n",
    "    # Process each pixel and accumulate histogram counts\n",
    "    for code in lbp_image.ravel():\n",
    "        typ, n, r = uniform_map[int(code)]\n",
    "        if typ == 'uniform':\n",
    "            if n == 0:\n",
    "                count_zero += 1\n",
    "            elif n == P:\n",
    "                count_allone += 1\n",
    "            else:\n",
    "                H_uniform[n][r] += 1\n",
    "        else:\n",
    "            count_nonuniform += 1\n",
    "    \n",
    "    # For each uniform pattern row, compute the FFT and keep first (P//2+1) coefficients\n",
    "    lbp_hf_features = []\n",
    "    num_coeff = P // 2 + 1\n",
    "    for n in range(1, P):\n",
    "        hist_row = H_uniform[n]\n",
    "        fft_coeff = np.fft.fft(hist_row)\n",
    "        lbp_hf_features.extend(np.abs(fft_coeff[:num_coeff]))\n",
    "    \n",
    "    # Append the extra 3 bins\n",
    "    extra_features = np.array([count_zero, count_allone, count_nonuniform], dtype=np.float32)\n",
    "    \n",
    "    feature_vector = np.concatenate((np.array(lbp_hf_features, dtype=np.float32), extra_features))\n",
    "    \n",
    "    # Normalize the final feature vector\n",
    "    if feature_vector.sum() > 0:\n",
    "        feature_vector = feature_vector / feature_vector.sum()\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# Example usage:\n",
    "# lbp_hf_features = lbp_hf_feature(query_img_gray)\n",
    "# print(lbp_hf_features)\n",
    "# print(lbp_hf_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2db491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(mtd_features, lbp_hf_features, w4=0.5, w5=0.5):\n",
    "    combined_features = np.concatenate([mtd_features * w4, lbp_hf_features * w5])\n",
    "    return combined_features\n",
    "\n",
    "# Example usage:\n",
    "# combined_features = combine_features(mtd_features, lbp_hf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe432130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test, model):\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"F1-score: {f1:.4f}\")\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3a05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(dataset_root, n_iter, model, done):\n",
    "    N_POINTS = 2048    \n",
    "    M_COEFFS = 5       \n",
    "    LBP_RADIUS = 3     \n",
    "    LBP_POINTS = 24    \n",
    "    \n",
    "    # Load features and labels\n",
    "    features, labels = [], []\n",
    "    if(done == 0):\n",
    "        for class_id in range(1, 101):  # t1 to t100 for CVIP100\n",
    "            class_dir = os.path.join(dataset_root, f't{class_id}')  \n",
    "            for img_file in sorted(os.listdir(class_dir)):  \n",
    "                img_path = os.path.join(class_dir, img_file)\n",
    "                _, gray, _, contour = load_and_preprocess_image(img_path)\n",
    "                \n",
    "                # Extract mtd and LBP-HF feature\n",
    "                mtd = mtd_feature(contour, N=N_POINTS, M=M_COEFFS)\n",
    "                lbp = lbp_hf_feature(gray, P=LBP_POINTS, R=LBP_RADIUS)\n",
    "                \n",
    "                features.append(combine_features(mtd, lbp))\n",
    "                # features.append(mtd)\n",
    "                # features.append(lbp)\n",
    "                labels.append(class_id-1)\n",
    "        np.save('CVIP100_concat_features.npy', features)\n",
    "        np.save('CVIP100_labels.npy', labels)\n",
    "    else:\n",
    "        features = np.load('CVIP100_concat_features.npy')\n",
    "        \n",
    "        labels = np.load('CVIP100_labels.npy')\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    acc_score_list = []\n",
    "    f1_score_list = []\n",
    "    def iteration(n_iter, features, labels, model):\n",
    "        for i in range(n_iter):\n",
    "            # train and test dataset split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                features, labels, test_size=0.3, stratify=None, random_state=i)\n",
    "            acc, f1= train_and_evaluate(X_train, X_test, y_train, y_test, model)\n",
    "            acc_score_list.append(acc)\n",
    "            f1_score_list.append(f1)\n",
    "        return np.mean(acc_score_list), np.mean(f1_score_list)\n",
    "\n",
    "    return iteration(n_iter, features, labels, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1021d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Result Final Accuracy: 0.9354 Final F1-score: 0.9352\n",
      "SVM Result Final Accuracy: 0.9618 Final F1-score: 0.9617\n",
      "Gaussian Naive Bayes Result Final Accuracy: 0.8975 Final F1-score: 0.8983\n",
      "Logistic Regression Result Final Accuracy: 0.9644 Final F1-score: 0.9647\n",
      "Decision Tree Result Final Accuracy: 0.6056 Final F1-score: 0.5998\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "n_iter = 20\n",
    "dataset_root = r'dataset/CVIP100 Leaf Dataset'        \n",
    "# KNN\n",
    "accuracy, f1 = full_pipeline(dataset_root, n_iter, KNeighborsClassifier(n_neighbors=1),0)\n",
    "print(f\"KNN Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n",
    "\n",
    "# SVM\n",
    "accuracy, f1 = full_pipeline(dataset_root, n_iter, SVC(kernel='rbf', C=10, gamma=0.001),1)\n",
    "print(f\"SVM Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n",
    "\n",
    "# Naive Bayes\n",
    "accuracy, f1 = full_pipeline(dataset_root, n_iter, GaussianNB(),1)\n",
    "print(f\"Gaussian Naive Bayes Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n",
    "\n",
    "accuracy, f1 = full_pipeline(dataset_root, n_iter, LogisticRegression(max_iter=1000),1)\n",
    "print(f\"Logistic Regression Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n",
    "\n",
    "accuracy, f1 = full_pipeline(dataset_root, n_iter, DecisionTreeClassifier(),1)\n",
    "print(f\"Decision Tree Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d5bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline_swedish(dataset_root, n_iter, model,done):\n",
    "    N_POINTS = 2048    \n",
    "    M_COEFFS = 5       \n",
    "    LBP_RADIUS = 3     \n",
    "    LBP_POINTS = 24    \n",
    "    \n",
    "    # Load features and labels\n",
    "    features, labels = [], []\n",
    "    if(done == 0):\n",
    "        for class_id in range(1, 16):  # t1 to t100 for CVIP100\n",
    "            class_dir = os.path.join(dataset_root, f'leaf{class_id}')  \n",
    "            for img_file in sorted(os.listdir(class_dir)):  \n",
    "                img_path = os.path.join(class_dir, img_file)\n",
    "                _, gray, _, contour = load_and_preprocess_image(img_path)\n",
    "                \n",
    "                # Extract mtd and LBP-HF feature\n",
    "                mtd = mtd_feature(contour, N=N_POINTS, M=M_COEFFS)\n",
    "                lbp = lbp_hf_feature(gray, P=LBP_POINTS, R=LBP_RADIUS)\n",
    "                \n",
    "                features.append(combine_features(mtd, lbp))\n",
    "                # features.append(mtd)\n",
    "                # features.append(lbp)\n",
    "                labels.append(class_id-1)\n",
    "        np.save('swedish_concat_features.npy', features)\n",
    "        np.save('swedish_labels.npy', labels)\n",
    "    else:\n",
    "        features = np.load('swedish_concat_features.npy')\n",
    "        labels = np.load('swedish_labels.npy')\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    acc_score_list = []\n",
    "    f1_score_list = []\n",
    "    def iteration(n_iter, features, labels, model):\n",
    "        for i in range(n_iter):\n",
    "            # train and test dataset split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                features, labels, test_size=0.3, stratify=None, random_state=i)\n",
    "            acc, f1= train_and_evaluate(X_train, X_test, y_train, y_test, model)\n",
    "            acc_score_list.append(acc)\n",
    "            f1_score_list.append(f1)\n",
    "        return np.mean(acc_score_list), np.mean(f1_score_list)\n",
    "\n",
    "    return iteration(n_iter, features, labels, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601e5376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Result Final Accuracy: 0.9682 Final F1-score: 0.9682\n",
      "SVM Result Final Accuracy: 0.9864 Final F1-score: 0.9864\n",
      "Gaussian Naive Bayes Result Final Accuracy: 0.9330 Final F1-score: 0.9346\n",
      "Logistic Regression Result Final Accuracy: 0.9874 Final F1-score: 0.9874\n",
      "Decision Tree Result Final Accuracy: 0.8879 Final F1-score: 0.8886\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "n_iter = 20\n",
    "dataset_root = r'dataset/Swedish Leaf Dataset'        \n",
    "# KNN\n",
    "accuracy, f1 = full_pipeline_swedish(dataset_root, n_iter, KNeighborsClassifier(n_neighbors=1),0)\n",
    "print(f\"KNN Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n",
    "\n",
    "# SVM\n",
    "accuracy, f1 = full_pipeline_swedish(dataset_root, n_iter, SVC(kernel='rbf', C=10, gamma=0.001),1)\n",
    "print(f\"SVM Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n",
    "\n",
    "# Naive Bayes\n",
    "accuracy, f1 = full_pipeline_swedish(dataset_root, n_iter, GaussianNB(),1)\n",
    "print(f\"Gaussian Naive Bayes Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n",
    "\n",
    "accuracy, f1 = full_pipeline_swedish(dataset_root, n_iter, LogisticRegression(max_iter=1000),1)\n",
    "print(f\"Logistic Regression Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n",
    "\n",
    "accuracy, f1 = full_pipeline_swedish(dataset_root, n_iter, DecisionTreeClassifier(),1)\n",
    "print(f\"Decision Tree Result Final Accuracy: {accuracy:.4f} Final F1-score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageproc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
